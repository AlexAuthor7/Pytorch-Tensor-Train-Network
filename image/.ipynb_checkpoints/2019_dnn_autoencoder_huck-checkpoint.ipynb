{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import argparse \n",
    "import time \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size =200\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "\n",
    "def param(nnet, Mb=True):\n",
    "    \"\"\"\n",
    "    Return the number of parameters in nnet\n",
    "    \"\"\"\n",
    "    nelems = sum([param.nelement() for param in nnet.parameters()])\n",
    "\n",
    "    return nelems / 10**6 if Mb else nelems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "Num_H = 1600\n",
    "\n",
    "class dnn_autoencoder(nn.Module):\n",
    "    def __init__(self,Num_H):\n",
    "        super(dnn_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, Num_H),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(Num_H, Num_H),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(Num_H, 28*28), nn.ReLU(True))\n",
    "        self.name = \"DNN_autoencoder\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ae(model, criterion, optimizer, test_loader):\n",
    "    # monitor training loss\n",
    "    test_loss = 0.0\n",
    "    for data in test_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        test_loss += loss.item()*images.size(0)\n",
    "\n",
    "    test_loss = test_loss/len(test_loader)\n",
    "    print('Val. Loss: {:.6f}'.format( \n",
    "        test_loss\n",
    "        ))\n",
    "    return test_loss\n",
    "\n",
    "def train_ae(model, n_epochs, criterion, optimizer, train_loader, test_loader, w_name = \"19_dnn_ae\"):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for data in train_loader:\n",
    "            # _ stands in for labels, here\n",
    "            images, _ = data\n",
    "            # flatten images\n",
    "            images = images.view(images.size(0), -1)\n",
    "            images = images.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, images)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*images.size(0)\n",
    "\n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        test_loss = test_ae(model, criterion, optimizer, test_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f} \\t Val. Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss, test_loss\n",
    "            ))\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_weights/\" + w_name + \".pt\")\n",
    "    return model, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### get data\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "# Create training and test dataloaders\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "lr = 0.0001\n",
    "# specify loss function\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_res(model):\n",
    "    # obtain one batch of test images\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    images_flatten = images.view(images.size(0), -1)\n",
    "    # get sample outputs\n",
    "    output = model(images_flatten.to(device))\n",
    "    # prep images for display\n",
    "    images = images.numpy()\n",
    "\n",
    "    # output is resized into a batch of images\n",
    "    output = output.view(batch_size, 1, 28, 28)\n",
    "    # use detach when it's an output that requires_grad\n",
    "    output = output.cpu().detach().numpy()\n",
    "\n",
    "    # plot the first ten input images and then reconstructed images\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "    # input images on top row, reconstructions on bottom\n",
    "    for images, row in zip([images, output], axes):\n",
    "        for img, ax in zip(images, row):\n",
    "            ax.imshow(np.squeeze(img), cmap='gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ae = autoencoder(800).cuda()\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(\n",
    "# model_ae.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# model_ae = train_ae(model_ae, 10, criterion, optimizer, train_loader)\n",
    "# loss = train_ae(model_ae, criterion, optimizer, test_loader)\n",
    "\n",
    "# print(\"DNN MSE Results, N = 800\")\n",
    "# print(\"DNN MSE\", param(model_ae),\"MB\", \", Val. loss = \", loss)\n",
    "# show_res(model_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-205c423686a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNum_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNum_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = torch.optim.Adam(\n",
      "\u001b[0;32m<ipython-input-2-b576e088c565>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Num_H)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mdnn_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNum_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         self.encoder = nn.Sequential(\n\u001b[1;32m     26\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNum_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "Num_H = 800\n",
    "\n",
    "model_ae = dnn_autoencoder(Num_H).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(\n",
    "model_ae.parameters(), lr=1e-4)\n",
    "\n",
    "model_ae, val_loss = train_ae(model_ae, 10, criterion, optimizer, train_loader, test_loader, w_name = \"ann_ae\"+str(Num_H) )\n",
    "\n",
    "print(\"DNN MAE Results, N = 800\")\n",
    "print(\"DNN MAE\", param(model_ae),\"MB\", \", Val. loss = \", val_loss)\n",
    "show_res(model_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Num_H in [800, 900, 1000, 1100, 1600, 2400]:\n",
    "\n",
    "    model_ae = autoencoder(Num_H).cuda()\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "    model_ae.parameters(), lr=1e-4)\n",
    "\n",
    "    model_ae , val_loss = train_ae(model_ae, 10, criterion, optimizer, train_loader, test_loader, w_name = \"ann_ae\"+str(Num_H) )\n",
    "\n",
    "\n",
    "    print(\"DNN MAE Results, N =\", Num_H)\n",
    "    print(\"DNN MAE\", param(model_ae),\"MB\", \", Val. loss = \", val_loss)\n",
    "    show_res(model_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:speech1] *",
   "language": "python",
   "name": "conda-env-speech1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
